[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/3DbKuh4a)

# Dialogue Summarization | ì¼ìƒ ëŒ€í™” ìš”ì•½

## Team NLP 4ì¡°

| ![ë°•íŒ¨ìº ](https://avatars.githubusercontent.com/u/156163982?v=4) | ![ì´íŒ¨ìº ](https://avatars.githubusercontent.com/u/156163982?v=4) | ![ìµœíŒ¨ìº ](https://avatars.githubusercontent.com/u/156163982?v=4) | ![ê¹€íŒ¨ìº ](https://avatars.githubusercontent.com/u/156163982?v=4) | ![ì˜¤íŒ¨ìº ](https://avatars.githubusercontent.com/u/156163982?v=4) | ![ì˜¤íŒ¨ìº ](https://avatars.githubusercontent.com/u/156163982?v=4) |
|:-------------------------------------------------------------:|:-------------------------------------------------------------:|:-------------------------------------------------------------:|:-------------------------------------------------------------:|:-------------------------------------------------------------:|:-------------------------------------------------------------:|
| [ê°•ìŠ¹í˜„](https://github.com/UpstageAILab)                        | [ê¹€í˜•ìˆ˜](https://github.com/UpstageAILab)                        | [ì´ì†Œì˜A](https://github.com/UpstageAILab)                       | [ê¹€ì°½í¬](https://github.com/UpstageAILab)                        | [ì§„ì„±ì¤€](https://github.com/UpstageAILab)                        | [ê¹€í•˜ì—°](https://github.com/UpstageAILab)                        |
| íŒ€ì¥                                                            | íŒ€ì›                                                            | íŒ€ì›                                                            | íŒ€ì›                                                            | íŒ€ì›                                                            | íŒ€ì›                                                            |

## 0. Overview

### Environment

- **(ì»´í“¨íŒ… í™˜ê²½)** ì¸ë‹¹ RTX 3090 ì„œë²„ë¥¼ VSCodeì™€ SSHë¡œ ì—°ê²°í•˜ì—¬ ì‚¬ìš©
- **(í˜‘ì—… í™˜ê²½)** Github, Wandb
- **(ì˜ì‚¬ ì†Œí†µ)** Slack, Zoom

## 1. Competiton Info

### Overview

- ì´ë²ˆ ëŒ€íšŒëŠ” Dialogue Summurization ëŒ€íšŒë¡œ ì¼ìƒ ëŒ€í™”ë¡œ ì´ë£¨ì–´ì§„ ëŒ€í™”ë¬¸ì˜ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ëŒ€íšŒì…ë‹ˆë‹¤.

### Timeline

- March 08, 2024 - Start Date
- March 20, 2024 - Final submission deadline

## 2. Components

### Directory

```
â”œâ”€â”€ README.md
â”œâ”€â”€ changhee
â”‚   â”œâ”€â”€ DS-T5.ipynb
â”‚   â””â”€â”€ cleaning_data.ipynb
â”œâ”€â”€ hayeon
â”‚   â”œâ”€â”€ kobart_gogamza_keyword.ipynb
â”‚   â”œâ”€â”€ kot5_keyword.ipynb
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ hyperparameter-tuning
â”‚   â”œâ”€â”€ hp_tuning_ver_2
â”‚   â”œâ”€â”€ hp_tuning_ver_3.ipynb
â”‚   â””â”€â”€ optuna_hsk.ipynb
â”œâ”€â”€ seongjun
â”‚   â””â”€â”€ t501_lora_inference.ipynb
â””â”€â”€ soyoung
    â””â”€â”€ baseline_kot5.ipynb
```

## 3. Data descrption

### Dataset overview

- ì´ë²ˆ ëŒ€íšŒì— ì‚¬ìš©ëœ ë°ì´í„°ëŠ” ëª¨ë‘ ëŒ€í™”ë¬¸ ë°ì´í„°ì´ë©° train data, validation data, test dataëŠ” ê°ê° 12,457ê°œ, 499ê°œ, 499ê°œ ì…ë‹ˆë‹¤. ê°ê°ì˜ ëŒ€í™”ë¬¸ì€ ìµœì†Œ 2ëª…ì—ì„œ ìµœëŒ€ 7ëª…ì˜ í™”ìê°€ ë“±ì¥í•˜ë©°, ìµœì†Œ 2í„´ì—ì„œ ìµœëŒ€ 60í„´ê¹Œì§€ ëŒ€í™”ê°€ ì´ì–´ì§‘ë‹ˆë‹¤. ëŒ€í™”ë¬¸ì—ì„œ ë°œí™”ìëŠ” #Person"N"#ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ìˆìŠµë‹ˆë‹¤.
- ëŒ€í™”ë¬¸ì— ì¡´ì¬í•˜ëŠ” ê°œì¸ì •ë³´(ì˜ˆ: ì „í™”ë²ˆí˜¸, ì£¼ì†Œ ë“±)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§ˆìŠ¤í‚¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
  - ì˜ˆ) ì „í™”ë²ˆí˜¸ -> #PhoneNumber#

### EDA

- Train data, validation data, test data ì— ì¡´ì¬í•˜ëŠ” #..# í˜•ì‹ì˜ ë‹¨ì–´: 
  
  - `['#Address#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#PassportNumber#', '#Person#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', '#SSN#']`

- Train data, validation data, test dataë¥¼ `kobart-summarization`ìœ¼ë¡œ tokenized í–ˆì„ ë•Œ ê° ëŒ€í™”ë¬¸ê³¼ ìš”ì•½ë¬¸ì˜ ê¸¸ì´ ì •ë³´
  
  |         | **Train Data**                                                                   | **Validation Data**                                                              | **Test Data**                                                                     |
  | ------- |:-------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
  | **ëŒ€í™”ë¬¸** | mean:Â 158.597014<br/>min: 32<br/>25%: 105<br/>50%: 141<br/>75%: 197<br/>max: 950 | mean:Â 156.687375<br/>min: 46<br/>25%: 104<br/>50%: 139<br/>75%: 194<br/>max: 620 | mean:Â 163.146293<br/>min: 38<br/>25%: 103<br/>50%: 149<br/>75%: 204<br/>max: 1041 |
  | **ìš”ì•½ë¬¸** | mean:Â 30.980734<br/>min: 7<br/>25%: 21<br/>50%: 29<br/>75%: 38<br/>max: 165      | mean:Â 28.795591<br/>min: 9<br/>25%: 19<br/>50%: 26<br/>75%: 35<br/>max: 89       |                                                                                   |

### Data Processing

- ì˜¤íƒ€ ìˆ˜ì •
  
  - '...#Person2#: ë¨¼ì €, ì´ê²ƒì€ 19ì„¸ê¸° ì´ˆ ë°°ê²½ã…‡ë¡œ ì„¤ì •ëœ ë¡œë§¨ìŠ¤ ì†Œì„¤ì´ì—ìš”...' -> â€˜ìœ¼ë¡œâ€™
  
  - '...#Person1#: í¸ì§‘ì¥ì´ ì œã… ë‹¤ë¥¸ ì¡ì§€ì—ì„œ í¸ì§‘ìë¡œ ì¼í–ˆë˜ ê²½í—˜ì´ ìˆë‹¤ëŠ” ê±¸ ë“£ê³ , ê·¸ê°€ ë„ìš°ë¯¸ í¸ì§‘ìê°€ ë˜ê³  ì‹¶ëƒê³  ë¬¼ì–´ë´¤ì–´ìš”....' -> â€˜ì œê°€â€™
  
  - '...#Person1#: ì´ì œ ê·¸ë§Œ. ë„ˆëŠ” ì•„ì§ã…ì•Œë§ëŠ” ì‚¬ëŒì„ ë§Œë‚˜ì§€ ëª»í–ˆì„ ë¿ì´ê³ , ë„ˆëŠ” ë„ˆë¬´ ë§ì´ ì¼í•˜ëŠ” ê²ƒ ê°™ì•„. ë„ˆëŠ” ì–´ë–»ê²Œ ì¦ê¸°ê³  ì‚¶ì„ ì¦ê¸°ëŠ” ë²•ì„ ë°°ì›Œì•¼ í•´....' -> â€˜ ì•Œë§ëŠ”â€™
  
  - '...#Person1#: ì•„ë¬´ê²ƒë„ ì•ˆ í–ˆì–´. ê·¸ëŠ” ê²°êµ­ ë‚˜ê°”ì–´. ê·¸ëŸ°ë° ì˜¤ëŠ˜ ë˜ ê·¸ë¥¼ ë´¤ì–´. ì‹ ë°œ ê°€ê²Œ ë°–ì—ì„œ. ì¹´í˜ ê·¼ì²˜ì—ì„œ. ë‚˜ëŠ” CD ê°€ê²Œì— ë“¤ì–´ê°€ì„œ CDë¥¼ ë³´ëŠ” ì²™í–ˆã„·ê±°ë“ . ê·¸ëŸ°ë° ê·¸ë„ ë“¤ì–´ì™”ì–´....' -> â€˜ ê±°ë“ '**

- ì—°ì†ëœ ììŒ ëŒ€ì²´
  
  - '...#Person1#: ì†ì•˜ì–´! ã…‹ã…‹.. ì™„ì „ ì†ì•˜ì–´....' -> â€˜ì›ƒê¸°ë‹¤â€™

- ë§ˆìŠ¤í‚¹ ì •ë³´ ì¶”ì¶œ -> í† í°í™” ì‹œ Special Tokenìœ¼ë¡œ ì¶”ê°€

## 4. Modeling

**ê°•ìŠ¹í˜„**

- ì‚¬ìš©í•œ ëª¨ë¸  
  
  - bart ê¸°ë°˜ í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ ëª¨ë¸  
    
    - `digit82/kobart-summarization`
    
    - `EbanLee/kobart-summary-v2`
    
    - `gangyeolkim/kobart-korean-summarizer-v2`
    
    - `ainize/kobart-news`
  
  - gpt2 ê¸°ë°˜ í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ ëª¨ë¸  
    
    - `digit82/kogpt2-summarization`

- ì‹¤í—˜ ê²°ê³¼
  
  - `digit82/kobart-summarization`
    
    - baseline ëª¨ë¸: eval_loss => 0.5629, ë¦¬ë”ë³´ë“œ ì ìˆ˜ => 41.4015
    
    - Special Token ì¶”ê°€: eval_loss => 0.5623, ë¦¬ë”ë³´ë“œ ì ìˆ˜ => 41.5824 (ì ìˆ˜ ìƒìŠ¹)
    
    - í…ìŠ¤íŠ¸ ì •ì œ(ìŠ¤íƒ‘ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ë‹¨ì–´ ì œê±°): eval_loss => 0.5619, ë¦¬ë”ë³´ë“œ ì ìˆ˜ => 41.7505 (ì ìˆ˜ ìƒìŠ¹)
  
  - `EbanLee/kobart-summary-v2`
    
    - eval/loss => 0.6062
  
  - `gogamza/kobart-summarization`
    
    - eval_loss => 0.5790, ë¦¬ë”ë³´ë“œ ì ìˆ˜ => 40.2952 (ì ìˆ˜ í•˜ë½)
  
  - `ainize/kobart-news`
    
    - val_loss => 0.6404, ë¦¬ë”ë³´ë“œ ì ìˆ˜ => 39.3084 (ì ìˆ˜ í•˜ë½)
  
  - `digit82/kogpt2-summarization`
    
    - ë¦¬ë”ë³´ë“œ ì ìˆ˜ => 27.5233 (ì ìˆ˜ í•˜ë½)

**ê¹€ì°½í¬**

- Hugging faceì˜ ëª¨ë¸ `psyche/KoT5-summarization`, `lcw99/t5-large-korean-text-summary`, `eenzeenee/t5-base-korean-summarization`ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸

- `psyche/KoT5-summarization`ì„ ì‚¬ìš©í–ˆì„ ë•Œ Train lossëŠ” 0, Validation Lossê°€ ê³ ì •ë˜ëŠ” ë¬¸ì œê°€ ìˆì–´ transformerì˜ trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ training functionì„ êµ¬í˜„í•˜ì—¬ í•™ìŠµì„ í•˜ì˜€ì§€ë§Œ baselineì˜ `digit82/kobart-summarization` ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•¨

- `lcw99/t5-large-korean-text-summary`, `eenzeenee/t5-base-korean-summarization`ë¥¼ ì‚¬ìš©í•˜ì˜€ì„ ë•Œ Validation Lossê°€ ê°€ì¥ ë‚®ê²Œ ë‚˜ì˜¤ëŠ” `lcw99/t5-large-korean-text-summary`ëª¨ë¸ì„ ì„ íƒ

**ê¹€í•˜ì—°**

- ìœ ì´ì¬(2023). Improvement of Dialogue Summarization Using Keyword Extractor ì°¸ê³ , í‚¤ì›Œë“œ ê¸°ë°˜ ìƒì„± ìš”ì•½ ì§„í–‰

- pretrained model : gogamza/kobart-summarization

- Dialogue, Topicì„ í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ Test Topic ìƒì„±, Dialogue, Topic, Summaryë¥¼ í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ Test Summary ìƒì„±
  
  - ìµœì¢… ê²°ê³¼: `final_result` => 41.8308, `rouge1` => 0.5135, `rouge2` => 0.3181, `rougeL` => 0.4233

**ê¹€í˜•ìˆ˜**

- EDA ê³¼ì •ì—ì„œ ë°ì´í„°ì…‹ì˜ ëŒ€í™”ë¬¸ê³¼ ìš”ì•½ë¬¸ ìµœëŒ€ ê¸¸ì´ê°€ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì˜ `encoder_max_len`ì™€ `decoder_max_len`ë³´ë‹¤ ê¸´ ê²ƒì„ í™•ì¸í•˜ê³ , `encoder_max_len`, `decoder_max_len`, `generation_max_length` ê°’ì„ ì¡°ì ˆí•´ê°€ë©° í•™ìŠµ. 
  
  - ìµœì´ˆ `final_result` 41.6394ì—ì„œ 42.0135ë¡œ ì ìˆ˜ ì†Œí­ ìƒìŠ¹

- ì¶”ê°€ë¡œ ë” ë§ì€ `Seq2SeqTrainer`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•˜ì—¬ íŠœë‹í•˜ê¸° ìœ„í•˜ì—¬ `optuna` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

- `Trainer` í´ë˜ìŠ¤ì˜ `hyperparameter_search` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°`learning_rate`, `per_device_train_batch_size`, `per_device_eval_batch_size`, `weight_decay`, `lr_scheduler_type`, `optim`ë¥¼ íŠœë‹.
  
  - ìµœì¢… ê²°ê³¼: `final_result`=> 42.1127

**ì´ì†Œì˜**

- ì‚¬ìš©í•œ ëª¨ë¸  
  - bart ê¸°ë°˜ì˜ í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ ëª¨ë¸  
    - `digit82/kobart-summarization`  
  - t5 ê¸°ë°˜ì˜ í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ ëª¨ë¸  
    - `paust/pko-t5-base`  
    - `lcw99/t5-base-korean-text-summary`  
    - `noahkim/KoT5_news_summarization`  
    - `paust/pko-t5-large`  
    - `lcw99/t5-large-korean-text-summary`
- Modeling Process
  - ê°€ì„¤1: eda ê²°ê³¼ë¥¼ í•™ìŠµ ë° í‰ê°€ ë°ì´í„°ì˜ í† í° ê°œìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ encoder_max_len, decoder_max_len, generation_max_lengthë¥¼ ì¦ê°€ì‹œí‚¤ë©´ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒ
    - Model: digit82/kobart-summarization  
      Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â encoder_max_len: 1000  
      decoder_max_len: 200  
      per_device_train_batch_size: 16  
      -> ì ìˆ˜ í–¥ìƒ, LB: 42.1735  
    - Model: digit82/kobart-summarization  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 16  
      -> ì ìˆ˜ í–¥ìƒ, LB: 42.4697
  - ê°€ì„¤2: learning rateë¥¼ ì¤„ì´ë©´ íŒŒë¼ë¯¸í„°ê°€ ë” ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì—¬ ìŠ¤ì½”ì–´ê°€ í–¥ìƒë  ê²ƒ
    - Model: digit82/kobart-summarization  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 16  
      learning_rate: 5-e6  
      -> ì ìˆ˜ í•˜ë½, LB: 41.1145
  - ê°€ì„¤3: weight decayë¥¼ ëŠ˜ë¦¬ë©´ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒ
    - Model: digit82/kobart-summarization  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 16  
      weight_decay: 0.02  
      -> ì ìˆ˜ í•˜ë½, LB: 41.5500
  - ê°€ì„¤4: Text-to-Text ë°©ì‹ìœ¼ë¡œ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ìƒê´€ ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” T5 ê¸°ë°˜ ëª¨ë¸ì´ ëŒ€í™”ë¬¸ ìš”ì•½ taskì—ì„œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒ  
    - Model: paust/pko-t5-base  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 6  
      -> ì ìˆ˜ ìƒìŠ¹, LB: 42.8954  
    - Model: lcw99/t5-base-korean-text-summary  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 6  
      -> ì ìˆ˜ ìƒìŠ¹, LB: 42.9973  
    - Model: noahkim/KoT5_news_summarization  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 6  
      -> ì ìˆ˜ ìƒìŠ¹, LB: 43.2997
  - ê°€ì„¤5: batch sizeë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ gradient accumulation stepsë¥¼ ëŠ˜ë ¤ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°©ì‹ìœ¼ë¡œ ë” í° ë°°ì¹˜ í¬ê¸°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ë©´ ìˆ˜ë ´ ì†ë„ê°€ í–¥ìƒë  ê²ƒ  
    - Model: noahkim/KoT5_news_summarization  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 7  
      -> ì ìˆ˜ í•˜ë½, LB: 42.5872  
    - Model: noahkim/KoT5_news_summarization  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 7  
      gradient_accumulation_steps: 4  
      -> ì ìˆ˜ í•˜ë½, LB: 42.4952
  - ê°€ì„¤6: ë” ê°•ë ¥í•œ t5 ëª¨ë¸ì¸ t5-large ê¸°ë°˜ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒ  
    - Model: lcw99/t5-large-korean-text-summary  
      encoder_max_len: 1000  
      decoder_max_len: 200  
      generation_max_length: 200  
      per_device_train_batch_size: 1  
      -> ì ìˆ˜ ìƒìŠ¹, LB: 43.7724

**ì§„ì„±ì¤€**

- ì´ì†Œì˜ë‹˜ ê»˜ì„œ ê³µìœ í•´ì£¼ì‹  ëª¨ë¸ í•™ìŠµ í›„ lora ì¶”ê°€ í•™ìŠµ ì‚¬ìš© ëª¨ë¸
  
  - `noahkim/KoT5_news_summarization`
    
    - `"generation_max_length"`: 200,
    - `"encoder_max_len"`: 1000,  
    - `"decoder_max_len"`: 2000,  
  
  - `LoRA`
    
    ```python
    from peft import LoraConfig, TaskType, get_peft_model, PeftModellora_config = LoraConfig(  
    r=32,  
    target_modules=["q","v"],  
    bias="none",  
    task_type=TaskType.SEQ_2_SEQ_LM  
    )  
    lora_model = get_peft_model(generate_model, lora_config)  
    ```

- ìµœì¢… ê²°ê³¼: `final_result` => 42.8141, `rouge2` => 0.3260,  `rougeL` => 0.4325

## 5. Result

### Leader Board

- **Leader Board**
  
  <p align="center">
  <img src = "https://github.com/UpstageAILab/upstage-nlp-summarization-nlp4/assets/73140315/2f04b6ec-6ecd-4f39-998f-da990752a622" width="90%" height="90%">
  </p>

- **final_result**: 41.4034
  
  **rouge1**: 0.5153
  
  **rouge2**: 0.3079
  
  **rougeL**: 0.4189

### Presentation

- [[íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤] Upstage AI Lab 1ê¸°_4ì¡°_NLP_ê²½ì§„ëŒ€íšŒ ë°œí‘œìë£Œ - Google Slides](https://docs.google.com/presentation/d/1cHfb_lUnwTv2N0KXznXokDiXKVmHmT4JePSsrKm-LFI/edit?usp=sharing)

### Meeting Log

- [Meeting Log](https://quickest-asterisk-75d.notion.site/d2f34ff6d4d2415a905b9bd781f6a549?v=49d734caa9ce4a51b480219e5b99ba04&pvs=4)

### Reference

- [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
- [Using hyperparameter-search in Trainer - #9 by sgugger - ğŸ¤—Transformers - Hugging Face Forums](https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/9?page=3)
- [Huggingface text classification optuna wandb | Kaggle](https://www.kaggle.com/code/mahmoudhamza/huggingface-text-classification-optuna-wandb)
